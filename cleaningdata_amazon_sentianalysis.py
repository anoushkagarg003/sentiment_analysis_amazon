# -*- coding: utf-8 -*-
"""cleaningdata_amazon_sentianalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19Z4taHe4rQzuW10KqqHsKuEEXGiZh-Qm
"""

import pandas as pd
df= pd.read_csv('output.csv')
df.head(10)

!pip install googletrans==3.1.0a0

from googletrans import Translator
def translate_text(text, target_language='en'):
    try:
        translator = Translator()
        translation = translator.translate(text, dest=target_language)
        translated_text = translation.text
        return translated_text
    except Exception as e:
        print(f"Error translating text: {e}")
        return None
df['translated_reviews']=df.Reviews.apply(translate_text)
print(df['translated_reviews'])

df['translated_titles']=df.Titles.apply(translate_text)

!pip install langdetect

from langdetect import detect

def detect_language(text):
    try:
        language_code = detect(text)
        return language_code
    except:
        return None  # Return None if language detection fails

# Example usage:
df['language_rev'] = df['translated_reviews'].apply(detect_language)
df['language_title']= df['translated_titles'].apply(detect_language)
df.head(10)

df['translated_text'] = df[['translated_reviews', 'translated_titles']].agg(' '.join, axis=1)
df= df.drop(['translated_reviews', 'translated_titles'], axis=1)

!pip install contractions
!pip install word2number
!pip install unidecode
!pip install autocorrect

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from spacy.lang.en import STOP_WORDS
import contractions
from word2number import w2n
from unidecode import unidecode
from autocorrect import Speller
from string import punctuation

nltk.download('punkt')
nltk.download('stopwords')

# 1. remove spaces, newlines
def remove_spaces(df):
    clean_text = df.replace('\\n', ' ').replace("\t", ' ').replace('\\', ' ')
    return clean_text

# 2. contraction mapping
def expand_text(df):
    expanded_text = contractions.fix(df)
    return expanded_text

# 3. handling accented character
def handling_accented(df):
    fixed_text = unidecode(df)
    return fixed_text

# 4. Cleaning
stopword_list = stopwords.words("english")
stopword_list.remove('no')
stopword_list.remove('nor')
stopword_list.remove('not')

def clean_data(df):
    tokens = word_tokenize(df)
    clean_text = [word.lower() for word in tokens if (word not in punctuation) and (word.lower() not in stopword_list) and (len(word) > 2) and (word.isalpha())]
    return clean_text

# 5. autocorrect
def autocorrection(df):
    spell = Speller(lang='en')
    corrected_text = spell(df)
    return corrected_text

# 6. lemmatization
def lemmatization(df):
    lemmatizer = WordNetLemmatizer()
    final_data = []
    for word in df:
        lemmatized_word = lemmatizer.lemmatize(word)
        final_data.append(lemmatized_word)
    return " ".join(final_data)

import nltk
nltk.download('wordnet')
nltk.download('omw-1.4')

def process_text(text):
    if isinstance(text, float):  # Check if the text is a float
        return str(text)  # Convert float to string
    return text

clean_text_train = df.translated_text.apply(process_text)

clean_text_train = clean_text_train.apply(remove_spaces)
clean_text_train = clean_text_train.apply(expand_text)
clean_text_train = clean_text_train.apply(handling_accented)
clean_text_train = clean_text_train.apply(clean_data)
clean_text_train = clean_text_train.apply(lemmatization)

print(clean_text_train)

import nltk
from nltk import ngrams, word_tokenize
from collections import Counter

nltk.download('punkt')

def ngram_extractor(text, ngram_range):
    if not isinstance(text, str):
        text = str(text)  # Convert non-string inputs to string
    tokens = word_tokenize(text)
    ngram = ngrams(tokens, ngram_range)
    ngram_list = [' '.join(ngram1) for ngram1 in ngram]
    return ngram_list

list_unigrams = clean_text_train.apply(lambda x: ngram_extractor(x, 1))

final_unigram = []
for unigram in list_unigrams:
    final_unigram.extend(unigram)

cnt = Counter(final_unigram).most_common(25)
print(cnt)

df['clean_text_train'] = clean_text_train
final_df=df[['Stars','clean_text_train']]
final_df

final_df.to_csv("clean_text.csv")
from google.colab import files
files.download('clean_text.csv')